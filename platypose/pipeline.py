import os

import torch
import torch.nn as nn
from tqdm import tqdm

from platypose.diffusion.fp16_util import MixedPrecisionTrainer
from platypose.diffusion.resample import UniformSampler
from platypose.platform import platform
from platypose.utils.model_util import create_model_and_diffusion
from platypose.utils.wandb import download_wandb_artefact


class SkeletonPipeline(nn.Module):
    def __init__(
        self,
        config=None,
    ):
        super().__init__()
        # Model settings
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        self.model_config = {
            "modeltype": "",
            "njoints": 17,
            "nfeats": 3,
            "num_actions": 1,
            "translation": True,
            "pose_rep": "rot6d",
            "glob": True,
            "glob_rot": True,
            "latent_dim": 512,
            "ff_size": 1024,
            "num_layers": 8,
            "num_heads": 4,
            "dropout": 0.1,
            "activation": "gelu",
            "data_rep": "h36m",
            "cond_mode": "skeleton",
            "cond_mask_prob": 0.1,
            "action_emb": "tensor",
            "arch": "trans_enc",
            "emb_trans_dec": False,
            "clip_version": "ViT-B/32",
            "dataset": "humanml",
            "noise_schedule": "cosine",
            "sigma_small": True,
            "lambda_rcxyz": 0.0,
            "lambda_vel": 0.0,
            "lambda_fc": 0.0,
        }

        if config is not None:
            self.model_config.update(config)

        self.model, self.diffusion = create_model_and_diffusion(self.model_config)

    def sample(
        self, energy_fn, energy_scale=1, num_samples=1, num_frames=1, num_substeps=1, ddim=False, init_pose=None, skip_timesteps=0
    ):
        """
        This function samples from a diffusion model using a given energy function and other optional parameters.

        :param energy_fn: The energy function used to calculate the energy of the samples generated by the model. It takes
        in the generated samples and returns a dict of energies of the samples
        :param energy_scale: The energy scale is a parameter that scales the energy function used in the sampling process.
        It can be used to adjust the importance of the energy function relative to other factors in the sampling process. A
        higher energy scale will result in a stronger influence of the energy function on the samples generated, defaults to
        1 (optional)
        :param num_samples: The number of samples to generate, defaults to 1 (optional)
        :return: a set of samples generated using the progressive sampling loop of the diffusion model. The samples are
        generated based on the given energy function and energy scale, and the number of samples is determined by the
        `num_samples` parameter. The samples are returned as a tensor of shape `(num_samples, 3, image_size, image_size)`.
        The function also has optional parameters for using alpha
        """
        sample_fn = self.diffusion.ddim_sample_loop_progressive if ddim else self.diffusion.p_sample_loop_progressive

        return sample_fn(
            self.model,
            # (num_samples, 17, 3, num_frames),
            (num_samples, num_frames, 51),
            clip_denoised=False,
            model_kwargs={"y": {"uncond": True}},
            skip_timesteps=skip_timesteps,  # 0 is the default value - i.e. don't skip any step
            init_image=init_pose,
            progress=False,
            energy_fn=energy_fn,
            energy_scale=energy_scale,
            num_substeps=num_substeps
        )

    @staticmethod
    def _get_state_dict(path_or_artefact: str, use_cache: bool = False):
        """
        Checks if path_or_artefact is a path if not tries to download the model from wandb
        :param path_or_artefact: path to model or wandb artifact
        :param use_cache: if true uses the cached model
        :return: state dict of model
        """

        cache_path = "./models/" + path_or_artefact
        # check if ends with .pt
        if not path_or_artefact.endswith(".pt"):
            cache_path = cache_path + ".pt"

        print("cache_path", cache_path)
        if os.path.exists(cache_path) and use_cache:
            print("Using cached model")
            return torch.load(cache_path, map_location="cpu")
        else:
            return download_wandb_artefact(path_or_artefact)

    @classmethod
    def from_pretrained(
        cls,
        diffusion_artefact="sinzlab/platypose/MDM_H36m_30_frames_50_steps:latest",
        finetune=False,
    ):
        """
        Loads a pretrained model from wandb
        :param diffusion_artefact: wandb artifact name
        :return: None
        """
        platypose = cls()

        state_dict = platypose._get_state_dict(diffusion_artefact)

        if finetune:
            # remove parameters that are part of `input_process` and `output_process`
            state_dict = {
                k: v
                for k, v in state_dict.items()
                if not (k.startswith("input_process") or k.startswith("output_process"))
            }

        # platypose.model.load_state_dict(state_dict)
        # platypose.requires_grad_(True).eval()
        # platypose.to(platypose.device)

        # load the model excluding the input and output process
        platypose.model.load_state_dict(state_dict, strict=False)

        platypose.requires_grad_(True).eval()
        platypose.to(platypose.device)

        # parallelize the model to multiple GPUs
        avaliable_gpus = torch.cuda.device_count()
        platypose.model = nn.DataParallel(
            platypose.model, device_ids=list(range(avaliable_gpus))
        )

        # if finetune:
        #     # freeze the weights that are not part of the input and output process
        #     for name, param in platypose.model.named_parameters():
        #         if not (name.startswith("input_process") or name.startswith("output_process")):
        #             param.requires_grad = False
        #         else:
        #             param.requires_grad = True

        return platypose

    @classmethod
    def pretrain(cls, dataloader, cfg):
        self = cls()#.from_pretrained("model000450000.pt", finetune=True)
        self.to(cfg.device)

        mp_trainer = MixedPrecisionTrainer(
            model=self.model,
            use_fp16=False,
            fp16_scale_growth=1e-3,
        )
        schedule_sampler = UniformSampler(self.diffusion)
        opt = torch.optim.AdamW(
            mp_trainer.master_params,
            lr=cfg.train.lr,
            weight_decay=cfg.train.weight_decay,
        )

        num_epochs = cfg.train.num_steps // len(dataloader) + 1
        checkpoint_interval = 100_000 // len(dataloader) + 1

        for epoch in range(num_epochs):
            print(f"Starting epoch {epoch}")
            pbar = tqdm(dataloader)
            for (
                # cam, gt_3D, input_2D_update, action, subject, scale, bb_box, cam_ind, start_3d
                gt_3D,
                subject,
                cameras,
            ) in pbar:
                # gt_3D[:, :, 0] = 0  # set the root to 0

                # batch = gt_3D.permute(0, 2, 3, 1)
                # batch = batch.to(cfg.device)
                batch = gt_3D.to(cfg.device)

                # randomly clip the batch to different lengths
                if cfg.train.augment_length:
                    length = torch.randint(1, batch.shape[1], (1,)).item()
                    # batch = batch[..., :length]
                    batch = batch[:, :length]

                cond = {
                    "y": {
                        "mask": torch.ones((batch.shape[0], 1)).to(cfg.device),
                        "uncond": True,
                    },
                }

                mp_trainer.zero_grad()
                for i in range(0, batch.shape[0], cfg.train.batch_size):
                    # Eliminates the microbatch feature
                    assert i == 0
                    micro = batch
                    micro_cond = cond
                    t, weights = schedule_sampler.sample(micro.shape[0], cfg.device)

                    losses = self.diffusion.training_losses(
                        self.model,
                        micro,  # [bs, ch, image_size, image_size]
                        t,  # [bs](int) sampled timesteps
                        model_kwargs=micro_cond,
                        dataset=dataloader.dataset,
                    )

                    loss = (losses["loss"] * weights).mean()

                    platform.log(
                        {
                            "loss": loss.item(),
                            # "vel_loss": losses["vel_mse"].mean().item(),
                            "mse_loss": losses["rot_mse"].mean().item(),
                            # "bone_consistency": losses["bone_const"].mean().item(),
                        }
                    )

                    # pbar.set_description(f"loss: {loss.item():.4f} | vel_loss: {losses['vel_mse'].mean().item():.4f} | mse_loss: {losses['rot_mse'].mean().item():.4f} | bone_consistency: {losses['bone_const'].mean().item():.4f}")
                    pbar.set_description(f"loss: {loss.item():.4f}")

                    mp_trainer.backward(loss)

                    if epoch % checkpoint_interval == 0:
                        checkpoint_path = f"./models/{cfg.model.name}.pt"
                        torch.save(
                            self.model.state_dict(),
                            checkpoint_path,
                        )

                        platform.save(checkpoint_path)

                mp_trainer.optimize(opt)

        return self
